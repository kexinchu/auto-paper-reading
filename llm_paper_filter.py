"""
åŸºäºLLMçš„æ™ºèƒ½è®ºæ–‡ç­›é€‰æ¨¡å—
ä½¿ç”¨ä¸»é¢˜æè¿°å’ŒLLMåˆ¤æ–­è®ºæ–‡ç›¸å…³æ€§
æ”¯æŒOpenAIå…¼å®¹APIï¼ˆOpenAIã€DeepSeekã€SiliconFlowã€Qwenç­‰ï¼‰

ç­›é€‰æµç¨‹ï¼š
1. å…³é”®è¯é¢„ç­›é€‰ï¼šå¿«é€Ÿè¿‡æ»¤æ˜æ˜¾ä¸ç›¸å…³çš„è®ºæ–‡
2. LLMç›¸å…³æ€§è¯„ä¼°ï¼šåˆ¤æ–­è®ºæ–‡ä¸ä¸»é¢˜çš„ç›¸å…³ç¨‹åº¦ï¼ˆ0-10åˆ†ï¼‰
3. LLMè´¨é‡è¯„ä¼°ï¼šåˆ¤æ–­è®ºæ–‡çš„å­¦æœ¯è´¨é‡å’Œåˆ›æ–°æ€§ï¼ˆ0-10åˆ†ï¼‰
4. ç»¼åˆè¯„åˆ†ï¼šæ ¹æ®ç›¸å…³æ€§å’Œè´¨é‡çš„åŠ æƒå¾—åˆ†å†³å®šæ˜¯å¦ä¿ç•™
"""

import yaml
import os
from typing import List, Dict, Tuple, Set
import logging
from datetime import datetime
from openai import OpenAI
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMPaperFilter:
    def __init__(self, config: Dict):
        self.config = config
        
        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è·å–APIå¯†é’¥
        api_key = os.getenv('OPENAI_API_KEY') or config.get('api_key', '')
        if not api_key or api_key == 'YOUR_API_KEY_HERE':
            logger.warning("æœªé…ç½®APIå¯†é’¥ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® api_key")
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼ˆæ”¯æŒå…¼å®¹æ¥å£ï¼‰
        self.client = OpenAI(
            api_key=api_key,
            base_url=config.get('api_base', 'https://api.openai.com/v1'),
            timeout=config.get('timeout', 30)
        )
        
        self.model = config.get('model', 'gpt-3.5-turbo')
        self.temperature = config.get('temperature', 0.1)
        self.max_tokens = config.get('max_tokens', 500)
        self.max_retries = config.get('max_retries', 3)
        self.retry_delay = config.get('retry_delay', 1)
        
        # åŠ è½½ä¸»é¢˜é…ç½®
        topics_data = self._load_topics()
        self.topics = topics_data.get('topics', [])
        self.filtering_config = topics_data.get('filtering', {})
        
        # ç­›é€‰å‚æ•°
        self.keyword_threshold = self.filtering_config.get('keyword_match_threshold', 0.1)
        self.min_relevance_score = self.filtering_config.get('min_relevance_score', 6)
        self.min_quality_score = self.filtering_config.get('min_quality_score', 6)
        self.relevance_weight = self.filtering_config.get('relevance_weight', 0.6)
        self.quality_weight = self.filtering_config.get('quality_weight', 0.4)
        self.min_combined_score = self.filtering_config.get('min_combined_score', 6.0)
        self.enable_quality = self.filtering_config.get('enable_quality_assessment', True)
        
        logger.info(f"ç­›é€‰é…ç½®: å…³é”®è¯é˜ˆå€¼={self.keyword_threshold}, "
                   f"ç›¸å…³æ€§é˜ˆå€¼={self.min_relevance_score}, "
                   f"è´¨é‡é˜ˆå€¼={self.min_quality_score}, "
                   f"ç»¼åˆè¯„åˆ†é˜ˆå€¼={self.min_combined_score}")
        
    def _load_topics(self) -> Dict:
        """åŠ è½½ä¸»é¢˜é…ç½®"""
        try:
            with open('topics.yaml', 'r', encoding='utf-8') as f:
                topics_config = yaml.safe_load(f)
            return topics_config
        except Exception as e:
            logger.error(f"åŠ è½½ä¸»é¢˜æ–‡ä»¶å¤±è´¥: {e}")
            return {'topics': [], 'filtering': {}}
    
    def _extract_keywords_from_paper(self, paper: Dict) -> Set[str]:
        """ä»è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦ä¸­æå–å…³é”®è¯ï¼ˆè½¬ä¸ºå°å†™ï¼‰"""
        text = (paper.get('title', '') + ' ' + paper.get('abstract', '')).lower()
        # ç®€å•çš„è¯æå–ï¼ˆå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæ–¹æ³•ï¼‰
        words = set(re.findall(r'\b\w+\b', text))
        return words
    
    def _calculate_keyword_match(self, paper: Dict, topic: Dict) -> Tuple[float, List[str]]:
        """
        è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
        è¿”å›ï¼š(åŒ¹é…æ¯”ä¾‹, åŒ¹é…åˆ°çš„å…³é”®è¯åˆ—è¡¨)
        """
        keywords = topic.get('keywords', [])
        required_keywords = topic.get('required_keywords', [])
        
        if not keywords:
            return 1.0, []  # å¦‚æœæ²¡æœ‰é…ç½®å…³é”®è¯ï¼Œåˆ™è·³è¿‡å…³é”®è¯ç­›é€‰
        
        paper_words = self._extract_keywords_from_paper(paper)
        
        # æ£€æŸ¥å¿…éœ€å…³é”®è¯
        if required_keywords:
            required_found = False
            for req_kw in required_keywords:
                if req_kw.lower() in paper_words or any(req_kw.lower() in word for word in paper_words):
                    required_found = True
                    break
            if not required_found:
                return 0.0, []
        
        # è®¡ç®—åŒ¹é…çš„å…³é”®è¯
        matched_keywords = []
        for keyword in keywords:
            keyword_lower = keyword.lower()
            # å®Œå…¨åŒ¹é…æˆ–éƒ¨åˆ†åŒ¹é…
            if keyword_lower in paper_words or any(keyword_lower in word for word in paper_words):
                matched_keywords.append(keyword)
        
        match_ratio = len(matched_keywords) / len(keywords) if keywords else 0
        return match_ratio, matched_keywords
    
    def _call_llm(self, prompt: str, max_retries: int = None) -> str:
        """è°ƒç”¨LLM APIï¼ˆOpenAIå…¼å®¹æ¥å£ï¼‰"""
        if max_retries is None:
            max_retries = self.max_retries
            
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡è¯„ä¼°åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå‡†ç¡®åˆ¤æ–­è®ºæ–‡çš„ç›¸å…³æ€§å’Œè´¨é‡ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )
                
                return response.choices[0].message.content.strip()
                    
            except Exception as e:
                logger.warning(f"LLM APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    import time
                    time.sleep(self.retry_delay)
                    
        return ""
    
    def _create_relevance_prompt(self, paper: Dict, topic: Dict) -> str:
        """åˆ›å»ºç›¸å…³æ€§è¯„ä¼°æç¤º"""
        topic_name = topic['name']
        topic_description = topic['description']
        
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹è®ºæ–‡æ˜¯å¦ä¸ä¸»é¢˜"{topic_name}"ç›¸å…³ã€‚

ä¸»é¢˜æè¿°: {topic_description}

è®ºæ–‡ä¿¡æ¯:
æ ‡é¢˜: {paper['title']}
æ‘˜è¦: {paper['abstract'][:1500]}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”:
ç›¸å…³æ€§è¯„åˆ†: [0-10çš„æ•´æ•°ï¼Œ10è¡¨ç¤ºé«˜åº¦ç›¸å…³ï¼Œ0è¡¨ç¤ºå®Œå…¨ä¸ç›¸å…³]
ç†ç”±: [ç®€è¦è¯´æ˜è¯„åˆ†ç†ç”±ï¼Œ1-2å¥è¯]

è¯„åˆ†æ ‡å‡†ï¼š
- 9-10åˆ†: ç›´æ¥è§£å†³è¯¥ä¸»é¢˜çš„æ ¸å¿ƒé—®é¢˜
- 7-8åˆ†: ä¸ä¸»é¢˜å¯†åˆ‡ç›¸å…³
- 5-6åˆ†: ä¸ä¸»é¢˜æœ‰ä¸€å®šç›¸å…³æ€§
- 3-4åˆ†: ç•¥å¾®ç›¸å…³
- 0-2åˆ†: åŸºæœ¬ä¸ç›¸å…³

åªè¿”å›è¯„åˆ†å’Œç†ç”±ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        return prompt
    
    def _create_quality_prompt(self, paper: Dict) -> str:
        """åˆ›å»ºè´¨é‡è¯„ä¼°æç¤º"""
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹è®ºæ–‡çš„å­¦æœ¯è´¨é‡å’Œåˆ›æ–°æ€§ï¼Œå¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦é˜…è¯»å…¨æ–‡ã€‚

è®ºæ–‡ä¿¡æ¯:
æ ‡é¢˜: {paper['title']}
æ‘˜è¦: {paper['abstract'][:1500]}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”:
è´¨é‡è¯„åˆ†: [0-10çš„æ•´æ•°ï¼Œ10è¡¨ç¤ºè´¨é‡æé«˜ï¼Œ0è¡¨ç¤ºè´¨é‡å¾ˆå·®]
éœ€è¦å…¨æ–‡: [æ˜¯/å¦]
ç†ç”±: [ç®€è¦è¯´æ˜è¯„åˆ†ç†ç”±å’Œæ˜¯å¦éœ€è¦å…¨æ–‡çš„åŸå› ï¼Œ2-3å¥è¯]

è¯„åˆ†æ ‡å‡†ï¼š
- 9-10åˆ†: é‡å¤§çªç ´æ€§ç ”ç©¶ï¼Œæ–¹æ³•æ–°é¢–ï¼Œå®éªŒå……åˆ†ï¼Œå½±å“åŠ›å¤§
- 7-8åˆ†: åˆ›æ–°æ€§å¼ºï¼Œæ–¹æ³•å¯é ï¼Œå®éªŒå……åˆ†
- 5-6åˆ†: æœ‰ä¸€å®šåˆ›æ–°ï¼Œæ–¹æ³•åˆç†ï¼Œå®éªŒåŸºæœ¬å……åˆ†
- 3-4åˆ†: åˆ›æ–°æ€§ä¸€èˆ¬ï¼Œæ–¹æ³•å¸¸è§„ï¼Œå®éªŒä¸å¤Ÿå……åˆ†
- 0-2åˆ†: ç¼ºä¹åˆ›æ–°ï¼Œæ–¹æ³•é™ˆæ—§ï¼Œæˆ–å®éªŒä¸è¶³

æ˜¯å¦éœ€è¦å…¨æ–‡æ ‡å‡†ï¼š
- éœ€è¦å…¨æ–‡ï¼šæ‘˜è¦ä¿¡æ¯ä¸è¶³ä»¥ç†è§£æ ¸å¿ƒæ–¹æ³•ï¼Œæˆ–è€…æŠ€æœ¯ç»†èŠ‚éœ€è¦æ·±å…¥äº†è§£
- ä¸éœ€è¦ï¼šæ‘˜è¦å·²ç»æ¸…æ¥šè¯´æ˜äº†æ–¹æ³•å’Œç»“æœï¼Œæˆ–è€…è®ºæ–‡è´¨é‡/ç›¸å…³æ€§ä¸€èˆ¬

è¯„ä¼°è¦ç‚¹ï¼š
1. åˆ›æ–°æ€§ï¼šæ˜¯å¦æå‡ºæ–°æ–¹æ³•ã€æ–°æ€è·¯ï¼Ÿ
2. æ–¹æ³•å¯é æ€§ï¼šæ–¹æ³•æ˜¯å¦ç§‘å­¦ä¸¥è°¨ï¼Ÿ
3. å®éªŒå……åˆ†æ€§ï¼šå®éªŒæ˜¯å¦å…¨é¢ã€å¯¹æ¯”æ˜¯å¦å……åˆ†ï¼Ÿ
4. å®é™…ä»·å€¼ï¼šè§£å†³çš„é—®é¢˜æ˜¯å¦é‡è¦ï¼Ÿ
5. æ‘˜è¦å®Œæ•´æ€§ï¼šæ‘˜è¦æ˜¯å¦å·²ç»åŒ…å«è¶³å¤Ÿä¿¡æ¯ï¼Ÿ

åªè¿”å›è¯„åˆ†ã€æ˜¯å¦éœ€è¦å…¨æ–‡å’Œç†ç”±ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        return prompt
    
    def _parse_llm_response(self, response: str, score_prefix: str = 'ç›¸å…³æ€§è¯„åˆ†') -> Tuple[int, str]:
        """è§£æLLMå“åº”ï¼ˆç›¸å…³æ€§è¯„ä¼°ï¼‰"""
        try:
            lines = response.strip().split('\n')
            score = 0
            reason = ""
            
            for line in lines:
                if score_prefix in line or 'è¯„åˆ†' in line:
                    score_text = line.split(':', 1)[-1].strip()
                    # æå–æ•°å­—
                    numbers = re.findall(r'\d+', score_text)
                    if numbers:
                        score = int(numbers[0])
                        score = max(0, min(10, score))  # é™åˆ¶åœ¨0-10èŒƒå›´
                elif 'ç†ç”±' in line or 'reason' in line.lower():
                    reason = line.split(':', 1)[-1].strip()
            
            return score, reason
        except Exception as e:
            logger.warning(f"è§£æLLMå“åº”å¤±è´¥: {e}")
            return 0, "è§£æå¤±è´¥"
    
    def _parse_quality_response(self, response: str) -> Tuple[int, bool, str]:
        """
        è§£æè´¨é‡è¯„ä¼°å“åº”
        è¿”å›ï¼š(è´¨é‡è¯„åˆ†, æ˜¯å¦éœ€è¦å…¨æ–‡, ç†ç”±)
        """
        try:
            lines = response.strip().split('\n')
            score = 0
            need_fulltext = False
            reason = ""
            
            for line in lines:
                if 'è´¨é‡è¯„åˆ†' in line or 'è¯„åˆ†' in line:
                    score_text = line.split(':', 1)[-1].strip()
                    numbers = re.findall(r'\d+', score_text)
                    if numbers:
                        score = int(numbers[0])
                        score = max(0, min(10, score))
                elif 'éœ€è¦å…¨æ–‡' in line or 'fulltext' in line.lower():
                    text = line.split(':', 1)[-1].strip().lower()
                    # åˆ¤æ–­æ˜¯å¦éœ€è¦å…¨æ–‡
                    need_fulltext = ('æ˜¯' in text or 'yes' in text or 'éœ€è¦' in text or 'true' in text)
                elif 'ç†ç”±' in line:
                    reason = line.split(':', 1)[-1].strip()
            
            return score, need_fulltext, reason
        except Exception as e:
            logger.warning(f"è§£æè´¨é‡è¯„ä¼°å“åº”å¤±è´¥: {e}")
            return 0, False, "è§£æå¤±è´¥"
    
    def _evaluate_paper(self, paper: Dict, topic: Dict) -> Dict:
        """
        è¯„ä¼°å•ç¯‡è®ºæ–‡
        è¿”å›ï¼šè¯„ä¼°ç»“æœå­—å…¸
        """
        result = {
            'relevance_score': 0,
            'relevance_reason': '',
            'quality_score': 0,
            'quality_reason': '',
            'combined_score': 0,
            'matched_keywords': [],
            'keyword_match_ratio': 0,
            'need_fulltext': False,  # æ˜¯å¦éœ€è¦ä¸‹è½½å…¨æ–‡
            'passed': False
        }
        
        # 1. å…³é”®è¯ç­›é€‰
        match_ratio, matched_kws = self._calculate_keyword_match(paper, topic)
        result['keyword_match_ratio'] = match_ratio
        result['matched_keywords'] = matched_kws
        
        logger.info(f"  å…³é”®è¯åŒ¹é…: {match_ratio:.2%} ({len(matched_kws)}/{len(topic.get('keywords', []))})")
        
        if match_ratio < self.keyword_threshold:
            logger.info(f"  å…³é”®è¯åŒ¹é…åº¦è¿‡ä½ï¼Œè·³è¿‡LLMè¯„ä¼°")
            return result
        
        # 2. LLMç›¸å…³æ€§è¯„ä¼°
        relevance_prompt = self._create_relevance_prompt(paper, topic)
        relevance_response = self._call_llm(relevance_prompt)
        
        if relevance_response:
            rel_score, rel_reason = self._parse_llm_response(relevance_response, 'ç›¸å…³æ€§è¯„åˆ†')
            result['relevance_score'] = rel_score
            result['relevance_reason'] = rel_reason
            logger.info(f"  ç›¸å…³æ€§è¯„åˆ†: {rel_score}/10")
        else:
            logger.warning("  LLMç›¸å…³æ€§è¯„ä¼°å¤±è´¥")
            return result
        
        # 3. LLMè´¨é‡è¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_quality and result['relevance_score'] >= self.min_relevance_score:
            quality_prompt = self._create_quality_prompt(paper)
            quality_response = self._call_llm(quality_prompt)
            
            if quality_response:
                qual_score, need_fulltext, qual_reason = self._parse_quality_response(quality_response)
                result['quality_score'] = qual_score
                result['quality_reason'] = qual_reason
                result['need_fulltext'] = need_fulltext
                logger.info(f"  è´¨é‡è¯„åˆ†: {qual_score}/10, éœ€è¦å…¨æ–‡: {'æ˜¯' if need_fulltext else 'å¦'}")
            else:
                logger.warning("  LLMè´¨é‡è¯„ä¼°å¤±è´¥")
                result['quality_score'] = self.min_quality_score  # é»˜è®¤åŠæ ¼åˆ†
                result['need_fulltext'] = False  # é»˜è®¤ä¸éœ€è¦å…¨æ–‡
        else:
            result['quality_score'] = 10  # å¦‚æœä¸è¯„ä¼°è´¨é‡ï¼Œç»™æ»¡åˆ†
            result['need_fulltext'] = False  # é»˜è®¤ä¸éœ€è¦å…¨æ–‡
        
        # 4. è®¡ç®—ç»¼åˆè¯„åˆ†
        combined_score = (result['relevance_score'] * self.relevance_weight + 
                         result['quality_score'] * self.quality_weight)
        result['combined_score'] = combined_score
        
        # 5. åˆ¤æ–­æ˜¯å¦é€šè¿‡
        passed = (result['relevance_score'] >= self.min_relevance_score and
                 result['quality_score'] >= self.min_quality_score and
                 combined_score >= self.min_combined_score)
        result['passed'] = passed
        
        logger.info(f"  ç»¼åˆè¯„åˆ†: {combined_score:.1f}/10, é€šè¿‡: {passed}")
        
        return result
    
    def filter_papers(self, papers: List[Dict]) -> List[Dict]:
        """
        ä½¿ç”¨å…³é”®è¯+LLMç­›é€‰ç›¸å…³è®ºæ–‡
        """
        if not papers:
            return []
            
        if not self.topics:
            logger.warning("æ²¡æœ‰é…ç½®ä¸»é¢˜ï¼Œè¿”å›æ‰€æœ‰è®ºæ–‡")
            return papers
        
        filtered_papers = []
        
        for i, paper in enumerate(papers):
            logger.info(f"\næ­£åœ¨è¯„ä¼°è®ºæ–‡ {i+1}/{len(papers)}: {paper['title'][:60]}...")
            
            best_result = None
            best_topic = None
            
            # å¯¹æ¯ä¸ªä¸»é¢˜è¿›è¡Œè¯„ä¼°
            for topic in self.topics:
                logger.info(f"è¯„ä¼°ä¸»é¢˜: {topic['name']}")
                result = self._evaluate_paper(paper, topic)
                
                if result['passed'] and (best_result is None or 
                                        result['combined_score'] > best_result['combined_score']):
                    best_result = result
                    best_topic = topic['name']
            
            # å¦‚æœé€šè¿‡ç­›é€‰ï¼Œåˆ™ä¿ç•™è®ºæ–‡
            if best_result and best_result['passed']:
                paper_copy = paper.copy()
                paper_copy['relevance_score'] = best_result['relevance_score']
                paper_copy['relevance_reason'] = best_result['relevance_reason']
                paper_copy['quality_score'] = best_result['quality_score']
                paper_copy['quality_reason'] = best_result['quality_reason']
                paper_copy['combined_score'] = best_result['combined_score']
                paper_copy['matched_topic'] = best_topic
                paper_copy['matched_keywords'] = best_result['matched_keywords']
                paper_copy['keyword_match_ratio'] = best_result['keyword_match_ratio']
                paper_copy['need_fulltext'] = best_result['need_fulltext']  # æ˜¯å¦éœ€è¦ä¸‹è½½å…¨æ–‡
                
                filtered_papers.append(paper_copy)
                fulltext_mark = "ğŸ“„ éœ€è¦å…¨æ–‡" if best_result['need_fulltext'] else "ğŸ“‹ ä»…æ‘˜è¦"
                logger.info(f"âœ… è®ºæ–‡é€šè¿‡ç­›é€‰ (ç›¸å…³æ€§: {best_result['relevance_score']}, "
                          f"è´¨é‡: {best_result['quality_score']}, "
                          f"ç»¼åˆ: {best_result['combined_score']:.1f}, "
                          f"{fulltext_mark}, "
                          f"ä¸»é¢˜: {best_topic})")
            else:
                logger.info(f"âŒ è®ºæ–‡æœªé€šè¿‡ç­›é€‰")
        
        logger.info(f"\nç­›é€‰å®Œæˆ: {len(filtered_papers)}/{len(papers)} ç¯‡è®ºæ–‡é€šè¿‡ç­›é€‰")
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        filtered_papers.sort(key=lambda x: x.get('combined_score', 0), reverse=True)
        
        return filtered_papers
    
    def test_llm_connection(self) -> bool:
        """æµ‹è¯•LLMè¿æ¥"""
        try:
            test_prompt = "è¯·å›ç­”: 1+1ç­‰äºå¤šå°‘ï¼Ÿåªéœ€å›ç­”æ•°å­—ã€‚"
            response = self._call_llm(test_prompt)
            if response:
                logger.info(f"LLMè¿æ¥æµ‹è¯•æˆåŠŸï¼Œå“åº”: {response}")
                return True
            else:
                logger.error("LLMè¿æ¥æµ‹è¯•å¤±è´¥: æ— å“åº”")
                return False
        except Exception as e:
            logger.error(f"LLMè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False


def test_llm_filter():
    """æµ‹è¯•LLMç­›é€‰åŠŸèƒ½"""
    # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶åŠ è½½é…ç½®
    try:
        with open('config.yaml', 'r', encoding='utf-8') as f:
            import yaml
            full_config = yaml.safe_load(f)
            config = full_config.get('llm', {})
    except:
        config = {
            'api_base': os.getenv('OPENAI_API_BASE', 'https://api.openai.com/v1'),
            'api_key': os.getenv('OPENAI_API_KEY', ''),
            'model': os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo'),
            'max_retries': 3,
            'retry_delay': 1
        }
    
    filter_obj = LLMPaperFilter(config)
    
    # æµ‹è¯•è¿æ¥
    if not filter_obj.test_llm_connection():
        print("âŒ LLMè¿æ¥æµ‹è¯•å¤±è´¥")
        return
    
    print("âœ… LLMè¿æ¥æµ‹è¯•æˆåŠŸ")
    
    # æµ‹è¯•è®ºæ–‡ç­›é€‰
    test_papers = [
        {
            'title': 'Efficient KV-Cache Optimization for Large Language Models',
            'abstract': 'This paper presents a novel approach to optimize the key-value cache in transformer-based large language models, significantly reducing memory usage while maintaining performance. We propose a dynamic cache management strategy that adaptively selects which tokens to keep in the cache.',
            'authors': ['John Doe', 'Jane Smith']
        },
        {
            'title': 'A Study of Butterfly Migration Patterns',
            'abstract': 'We analyze the migration patterns of monarch butterflies across North America, focusing on environmental factors that influence their journey. Our findings suggest that climate change is affecting traditional migration routes.',
            'authors': ['Alice Johnson']
        }
    ]
    
    filtered_papers = filter_obj.filter_papers(test_papers)
    print(f"\nç­›é€‰ç»“æœ: {len(filtered_papers)}/{len(test_papers)} ç¯‡è®ºæ–‡é€šè¿‡ç­›é€‰\n")
    
    for paper in filtered_papers:
        print(f"æ ‡é¢˜: {paper['title']}")
        print(f"ä¸»é¢˜: {paper['matched_topic']}")
        print(f"ç›¸å…³æ€§: {paper['relevance_score']}/10 - {paper['relevance_reason']}")
        if 'quality_score' in paper and paper['quality_score'] > 0:
            print(f"è´¨é‡: {paper['quality_score']}/10 - {paper['quality_reason']}")
        print(f"ç»¼åˆè¯„åˆ†: {paper['combined_score']:.1f}/10")
        print(f"åŒ¹é…å…³é”®è¯: {', '.join(paper['matched_keywords'])}")
        print()


if __name__ == "__main__":
    test_llm_filter()
